 Here's your one-shot Replit script to generate all missing model logic files inside:

bash
Copy
Edit
/backend/logic/
ðŸ“„ Paste this into a new Replit file (e.g., generate_logic_files.py)
Then run it once in the terminal.

python
Copy
Edit
import os

logic_path = "backend/logic"
os.makedirs(logic_path, exist_ok=True)

models = {
    "ridge_regression.py": """import pandas as pd
from sklearn.linear_model import Ridge
from sklearn.metrics import r2_score, mean_squared_error

def run(df: pd.DataFrame, params: dict = None):
    X = df.select_dtypes(include='number').dropna()
    y = X.iloc[:, 0]
    X = X.iloc[:, 1:]
    model = Ridge(alpha=params.get("alpha", 1.0) if params else 1.0)
    model.fit(X, y)
    preds = model.predict(X)
    return {
        "charts": {"predicted_vs_actual": list(zip(preds.tolist(), y.tolist()))},
        "stats": {"r2": round(r2_score(y, preds), 4), "mse": round(mean_squared_error(y, preds), 4)},
        "tables": {"coefficients": dict(zip(X.columns, model.coef_.tolist()))},
        "explanation": "Ridge Regression applies L2 regularization to penalize large coefficients and reduce overfitting."
    }
""",
    "lasso_regression.py": """import pandas as pd
from sklearn.linear_model import Lasso
from sklearn.metrics import r2_score, mean_squared_error

def run(df: pd.DataFrame, params: dict = None):
    X = df.select_dtypes(include='number').dropna()
    y = X.iloc[:, 0]
    X = X.iloc[:, 1:]
    model = Lasso(alpha=params.get("alpha", 1.0) if params else 1.0)
    model.fit(X, y)
    preds = model.predict(X)
    return {
        "charts": {"predicted_vs_actual": list(zip(preds.tolist(), y.tolist()))},
        "stats": {"r2": round(r2_score(y, preds), 4), "mse": round(mean_squared_error(y, preds), 4)},
        "tables": {"coefficients": dict(zip(X.columns, model.coef_.tolist()))},
        "explanation": "Lasso Regression uses L1 regularization to reduce overfitting and perform variable selection."
    }
""",
    "svr.py": """import pandas as pd
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

def run(df: pd.DataFrame, params: dict = None):
    X = df.select_dtypes(include='number').dropna()
    y = X.iloc[:, 0]
    X = X.iloc[:, 1:]
    model = SVR()
    model.fit(X, y)
    preds = model.predict(X)
    return {
        "charts": {"predicted_vs_actual": list(zip(preds.tolist(), y.tolist()))},
        "stats": {"r2": round(r2_score(y, preds), 4), "mse": round(mean_squared_error(y, preds), 4)},
        "tables": {},
        "explanation": "SVR finds a function within a margin to best fit the data, robust to outliers."
    }
""",
    "gradient_boosting_classifier.py": """import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

def run(df: pd.DataFrame, params: dict = None):
    X = df.select_dtypes(include='number').dropna()
    y = X.iloc[:, 0]
    X = X.iloc[:, 1:]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
    model = GradientBoostingClassifier()
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    return {
        "charts": {"feature_importances": dict(zip(X.columns, model.feature_importances_.tolist()))},
        "stats": {"accuracy": round(accuracy_score(y_test, preds), 4)},
        "tables": {"classification_report": classification_report(y_test, preds, output_dict=True)},
        "explanation": "Gradient Boosting builds an ensemble of weak learners by minimizing loss function through gradient descent."
    }
""",
    "gradient_boosting_regressor.py": """import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

def run(df: pd.DataFrame, params: dict = None):
    X = df.select_dtypes(include='number').dropna()
    y = X.iloc[:, 0]
    X = X.iloc[:, 1:]
    model = GradientBoostingRegressor()
    model.fit(X, y)
    preds = model.predict(X)
    return {
        "charts": {"predicted_vs_actual": list(zip(preds.tolist(), y.tolist()))},
        "stats": {"r2": round(r2_score(y, preds), 4), "mse": round(mean_squared_error(y, preds), 4)},
        "tables": {"feature_importances": dict(zip(X.columns, model.feature_importances_.tolist()))},
        "explanation": "Gradient Boosting Regressor builds an ensemble of models sequentially, reducing residual errors using decision trees."
    }
""",
    "kernel_pca.py": """import pandas as pd
from sklearn.decomposition import KernelPCA

def run(df: pd.DataFrame, params: dict = None):
    X = df.select_dtypes(include='number').dropna()
    kpca = KernelPCA(n_components=2, kernel="rbf")
    X_kpca = kpca.fit_transform(X)
    return {
        "charts": {"kpca_projection": X_kpca.tolist()},
        "stats": {},
        "tables": {},
        "explanation": "Kernel PCA is an extension of PCA that enables non-linear dimensionality reduction using kernel tricks."
    }
"""
}

for filename, content in models.items():
    with open(os.path.join(logic_path, filename), "w") as f:
        f.write(content)

print("âœ… All model logic files generated in backend/logic/")